# Capstone Project: TurtleBot3 Autonomous Navigation
**Authors:** Chinmay Samak and Tanmay Samak

## Disclaimer:

I certify that all the work and writing that I contributed to here is my own and not acquired from external sources. I have cited sources appropriately and paraphrased correctly. I have not shared my writing with other students (for individual assignments) and other students outside my group (for group assignments), nor have I acquired any written portion of this document from past or present students.

## Robot Setup:

<img align="left" style="padding-left: 10px; padding-right: 10px; padding-bottom: 10px" width="175" src="media/setup_robot.png">

- [`turtlebot3_burger.urdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3/turtlebot3_description/urdf/turtlebot3_burger.urdf) from [`turtlebot3_description`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3/turtlebot3_description) package was modified [to define the fixed links and joints](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3/turtlebot3_description/urdf/turtlebot3_burger.urdf#L196-L223) (static transforms) for the camera module for simulating TurtleBot3 Burger with a camera in Gazebo simulator.
- [`model.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf) from [`turtlebot3_gazebo`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo) package was modified [to define links, physical properties](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf#L318-L370) as well as the [fixed joints](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/models/turtlebot3_burger/model.sdf#L419-L435) (static transforms) for the camera module for simulating TurtleBot3 Burger with a camera in Gazebo simulator.

## Environment Setup:

<img align="left" style="padding-left: 10px; padding-right: 10px; padding-bottom: 10px" width="175" src="media/setup_environment_sim.png">

- [`capstone_project.world`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/capstone_project.world) was defined from scratch to setup the environment with [ground](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/models/ground), [walls](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/models/walls), [obstacles](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/models/obstacles), [traffic sign](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/models/traffic_sign) and mobile [AptilTag marker](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/models/apriltag). Issues such as unstable obstacles have been rectified with our implementation.
- [`capstone_project.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/capstone_project.sdf) was defined to spawn the [modified TurtleBot3 Burger with a camera](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/README.md#robot-setup) in the [`capstone_project.world`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/capstone_project.world) environment.&nbsp;

## Description:

The workspace for [`capstone_project`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project) includes multiple ROS2 packages for accessing camera frames, vision processing, AprilTag detection, etc. that act as "helper packages" to the main [`capstone_project`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project) package.

The ROS2 package [`capstone_project`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project) for this project hosts the following [Python scripts](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/capstone_project):
- [`wall_following_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/wall_following_sim.py) makes the robot follow walls by maintaining equal distance from them. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on frontal distance to collision (based on 0° laserscan reading) and relative distance from walls (based on difference in means of left and right laserscan sectors spanning 30° each) respectively for motion control. A simple finite state machine is implemented to account for inf scan measurements beyond LIDAR maximum range. The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable wall following application.
- [`obstacle_avoidance_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/obstacle_avoidance_sim.py) makes the robot avoid obstacles by maintaining a safe distance from them. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on frontal distance to collision (based on mean of frontal laserscan sector of 40°), distance to collision from oblique left and right sectors spanning 70° each, and distance to collision from left and right sectors spanning 55° each respectively for motion control. A pseudo-potential field method is implemented with a hierarchical 3 stage attention mechanism to act on close, fairly far and safely far obstacles. A finite state machine is implemented to account for turning left, turning right, going straight and cautiously rotating on the spot in case of too many obstacle in proximity. The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable obstacle avoidance application.
- [`line_following_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/line_following_sim.py) makes the robot perform line following operation. The robot is commanded to move with 0.05 m/s linear velocity. The incoming RGB camera frame (320x240 px) is cropped according to the region of interest (ROI) of 10 px height and entire width, converted to [HSV](https://en.wikipedia.org/wiki/HSL_and_HSV) color space, and masked into a binary image using upper and lower HSV thresholds for bright yellow color. The binary image is then used to calculate moments (weighted average of image pixel intensities) to detect cluster of pixels (i.e. blob), compute the centroid of this blob, and then this centroid is used to calculate error (deviation) of robot from line center. A PID controller (with FIFO integral anti-windup mechanism) operates on this error to accordingly command the robot's angular velocity (rad/s) to perform line following operation. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically variable line following application.
- [`stop_sign_detection_sim`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/stop_sign_detection_sim.py) makes the robot come to a complete stop for a brief amount of time (4 seconds) if a stop sign is detected within a threshold distance. The incoming RGB camera frame (320x240 px) is passed through a pretrained Tiny-YOLO V7 model in inference mode. The resultant object detections are filtered on the fly based upon their class, classification confidence, and area of the bounding box in image space (implicitly giving away the detection and localization information of the stop sign). A finite state machine is implemented to act on the live filtered detections and determine the exact instance to come to a complete stop based on the preset threshold bounding box area (making sure that the robot doesn't stop too near or too far away from the stop sign). The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable stop sign detection application.
- [`apriltag_tracking_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/apriltag_tracking_sim.py) makes the robot detect and track AprilTag markers. The incoming camera feed from the robot is fed to the AprilTag detection pipeline from which the relative 6D pose of the AprilTag marker with respect to the robot camera is extracted. A static rigid body homogeneous transform is defined and applied between robot and camera frames to account for their relative pose. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on the positive translational Z and negative translational X components of robot relative marker pose respectively for motion control. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically variable AprilTag tracking application.
- [`apriltag_teleop.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/apriltag_teleop.py) makes the AprilTag marker (attached to an invisible TurtleBot3) move around based on the linear and angular velocity commands given by human from standard computer keyboard interface. `w/x` keys increase/decrease linear velocity, `a/d` keys increase/decrease angular velocity and `s` key applies emergency brakes. The script is setup to spin indefinitely to perform dynamically variable AprilTag marker teleoperation as required/demanded.
- [`capstone_project_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/capstone_project_sim.py) makes the robot perform all the aforementioned tasks by autonomous mode-switching behavior solely depending upon salient cues from the environment (without any human intervention). The incoming laserscan pattern and camera frame features from the robot are analyzed so as to deduce the current functional requirement for the robot. The algorithm has been robustified against false detections (e.g. to activate line following mode, it is ensured that there are no obstructions within on either of the two sides and either front or behind the robot, detection confidence and bounding box area of stop sign is continuously analyzed to avoid stopping randomly, etc.). In terms of priority scheduling, stop sign obedience takes precedence (utmost priority) followed by line following followed by AprilTag tracking followed by obstacle avoidance followed by wall following. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically reliable execution of the capstone project.
- [`wall_following_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/wall_following_real.py) makes the robot follow walls by maintaining equal distance from them. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on frontal distance to collision (based on 0° laserscan reading) and relative distance from walls (based on difference in means of left and right laserscan sectors spanning 30° each) respectively for motion control. A simple finite state machine is implemented to account for 0 scan measurements beyond LIDAR maximum range. The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable wall following application.
- [`obstacle_avoidance_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/obstacle_avoidance_real.py) makes the robot avoid obstacles by maintaining a safe distance from them. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on frontal distance to collision (based on mean of frontal laserscan sector of 40°), distance to collision from oblique left and right sectors spanning 20° each, and distance to collision from left and right sectors spanning 40° each respectively for motion control. A pseudo-potential field method is implemented with a hierarchical 3 stage attention mechanism to act on close, fairly far and safely far obstacles. The sector sizes are different from simulation owing to the fact that real-world LIDAR will pick up objects other than just the walls or obstacles. A finite state machine is implemented to account for turning left, turning right, going straight and cautiously rotating on the spot in case of too many obstacle in proximity. The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable obstacle avoidance application.
- [`line_following_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/line_following_real.py) makes the robot perform line following operation. The robot is commanded to move with 0.05 m/s linear velocity. The incoming RGB camera frame (320x240 px) is uncompressed, cropped according to the region of interest (ROI) of 10 px height and entire width, converted to [HSV](https://en.wikipedia.org/wiki/HSL_and_HSV) color space, and masked into a binary image using upper and lower HSV thresholds for light (fluorescent) yellow color. The binary image is then used to calculate moments (weighted average of image pixel intensities) to detect cluster of pixels (i.e. blob), compute the centroid of this blob, and then this centroid is used to calculate error (deviation) of robot from line center. A PID controller (with FIFO integral anti-windup mechanism) operates on this error to accordingly command the robot's angular velocity (rad/s) to perform line following operation. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically variable line following application.
- [`stop_sign_detection_real`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/stop_sign_detection_real.py) makes the robot come to a complete stop for a brief amount of time (4 seconds) if a stop sign is detected within a threshold distance. The incoming compressed RGB camera frame (320x240 px) is uncompressed and passed through a pretrained Tiny-YOLO V7 model in inference mode. The resultant object detections are filtered on the fly based upon their class, classification confidence, and area of the bounding box in image space (implicitly giving away the detection and localization information of the stop sign). A finite state machine is implemented to act on the live filtered detections and determine the exact instance to come to a complete stop based on the preset threshold bounding box area (making sure that the robot doesn't stop too near or too far away from the stop sign). The script is setup to wait 4 seconds for simulation to initialize properly and then run for eternity to demonstrate dynamically variable stop sign detection application.
- [`apriltag_tracking_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/apriltag_tracking_real.py) makes the robot detect and track AprilTag markers. The incoming compressed camera feed from the robot is uncompressed and fed to the AprilTag detection pipeline from which the relative 6D pose of the AprilTag marker with respect to the robot camera is extracted. A static rigid body homogeneous transform is defined and applied between robot and camera frames to account for their relative pose. The robot makes use of de-coupled longitudinal and lateral PID controllers (with FIFO integral anti-windup mechanism) acting on the positive translational Z and negative translational X components of robot relative marker pose respectively for motion control. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically variable AprilTag tracking application.
- [`capstone_project_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/capstone_project_real.py) makes the robot perform all the aforementioned tasks by autonomous mode-switching behavior solely depending upon salient cues from the environment (without any human intervention). The incoming laserscan pattern and camera frame features from the robot are analyzed so as to deduce the current functional requirement for the robot. The algorithm has been robustified against false detections (e.g. to activate line following mode, it is ensured that there are no obstructions within 1.0 m on either of the two sides and either front or behind the robot, detection confidence and bounding box area of stop sign is continuously analyzed to avoid stopping randomly, etc.) to ensure seamless mode-switching under a multitude of environmental conditions. In terms of priority scheduling, stop sign obedience takes precedence (utmost priority) followed by line following followed by AprilTag tracking followed by obstacle avoidance followed by wall following. The script is setup to wait 4 seconds for everything to initialize properly and then run for eternity to demonstrate dynamically reliable execution of the capstone project.

The ROS2 package [`capstone_project`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project) for this project hosts the following [launch files](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/capstone_project/capstone_project/launch):
- [`project_world.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/project_world.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`capstone_project.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/capstone_project.sdf) as well as the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py).
- [`wall_following_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/wall_following_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`wall_following.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/wall_following.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`wall_following_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/wall_following_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`obstacle_avoidance_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/obstacle_avoidance_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`obstacle_avoidance.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/obstacle_avoidance.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`obstacle_avoidance_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/obstacle_avoidance_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`line_following_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/line_following_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`line_following.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/line_following.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`line_following_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/line_following_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`stop_sign_detection_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/stop_sign_detection_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`stop_sign_detection.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/stop_sign_detection.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`darknet_ros.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/launch/darknet_ros.launch.py) with [`capstone_sim.yaml`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/config/capstone_sim.yaml) configuration, the [`stop_sign_detection_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/stop_sign_detection_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`apriltag_tracking_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/apriltag_tracking_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`apriltag_tracking.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/apriltag_tracking.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`apriltag_node`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/src/AprilTagNode.cpp) with [tags_36h11.yaml](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/cfg/tags_36h11.yaml) configuration, the [`apriltag_tracking_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/apriltag_tracking_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`capstone_project_sim.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/capstone_project_sim.launch.py) launches [Gazebo simulator](https://gazebosim.org/home) with [`capstone_project.sdf`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/worlds/capstone_project.sdf), the [robot_state_publisher.launch.py](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/turtlebot3/turtlebot3_simulations/turtlebot3_gazebo/launch/robot_state_publisher.launch.py), the [`darknet_ros.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/launch/darknet_ros.launch.py) with [`capstone_sim.yaml`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/config/capstone_sim.yaml) configuration, the [`apriltag_node`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/src/AprilTagNode.cpp) with [tags_36h11.yaml](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/cfg/tags_36h11.yaml) configuration, the [`capstone_project_sim.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/capstone_project_sim.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`wall_following_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/wall_following_real.launch.py) launches the [`wall_following_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/wall_following_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`obstacle_avoidance_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/obstacle_avoidance_real.launch.py) launches the [`obstacle_avoidance_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/obstacle_avoidance_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`line_following_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/line_following_real.launch.py) republishes incoming [compressed images](http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CompressedImage.html) (used for reducing on-board computational burden significantly) on `image/compressed` topic to `image/uncompressed` topic as [uncompressed images](http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html), launches the [`line_following_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/line_following_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`stop_sign_detection_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/stop_sign_detection_real.launch.py) republishes incoming [compressed images](http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CompressedImage.html) (used for reducing on-board computational burden significantly) on `image/compressed` topic to `image/uncompressed` topic as [uncompressed images](http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html), launches the [`darknet_ros.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/launch/darknet_ros.launch.py) with [`capstone_real.yaml`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/config/capstone_real.yaml) configuration, the [`stop_sign_detection_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/stop_sign_detection_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`apriltag_tracking_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/apriltag_tracking_real.launch.py) republishes incoming [compressed images](http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CompressedImage.html) (used for reducing on-board computational burden significantly) on `image/compressed` topic to `image/uncompressed` topic as [uncompressed images](http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html), launches the [`apriltag_node`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/src/AprilTagNode.cpp) with [tags_36h11.yaml](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/cfg/tags_36h11.yaml) configuration, the [static_transform_publisher](https://github.com/ros2/geometry2/blob/rolling/tf2_ros/src/static_transform_broadcaster_program.cpp) to define relative transform between camera and robot, the [`apriltag_tracking_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/apriltag_tracking_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).
- [`capstone_project_real.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/launch/capstone_project_real.launch.py) republishes incoming [compressed images](http://docs.ros.org/en/melodic/api/sensor_msgs/html/msg/CompressedImage.html) (used for reducing on-board computational burden significantly) on `image/compressed` topic to `image/uncompressed` topic as [uncompressed images](http://docs.ros.org/en/noetic/api/sensor_msgs/html/msg/Image.html), launches the [`darknet_ros.launch.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/launch/darknet_ros.launch.py) with [`capstone_real.yaml`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/darknet_ros/darknet_ros/darknet_ros/config/capstone_real.yaml) configuration, the [`apriltag_node`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/src/AprilTagNode.cpp) with [tags_36h11.yaml](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/apriltag_ros/cfg/tags_36h11.yaml) configuration,  the [`capstone_project_real.py`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/capstone_project/capstone_project/capstone_project/capstone_project_real.py) script as well as an [RViz](https://github.com/ros2/rviz) window to visualize the camera feed, laserscan and odometry estimates of the robot as well as a relative transformation between robot and AprilTag marker(s).

## Practical Considerations:

- With TurtleBot3, it is either possible to subscribe to sensor data or publish actuator commands (but NOT both) when interfaced with ROS-2 over a single [`Quality of Service (QoS)`](https://docs.ros.org/en/foxy/Concepts/About-Quality-of-Service-Settings.html) profile of the underlying [Data Distribution Service (DDS) or Real-Time Publish Subscribe (RTPS)](https://design.ros2.org/articles/ros_on_dds.html) implementation (also, [ROS-2 supports multiple DDS/RTPS implementations](https://docs.ros.org/en/foxy/Concepts/About-Different-Middleware-Vendors.html)). Hence, we have created different QoS profiles for subscribing to sensor data (using `best effort` communication and small queue depth) and publishing actuator commands (using `reliable` communication and relatively large queue depth).
- Laserscan sector FOV and image processing thresholds will be different for simulation and real-world deployments since [Gazebo](https://gazebosim.org/home) simulator does not capture "truly" realistic characteristics of the sensors and rendering of the environment; not to mention that simulated world is "perfect" and comprises only walls, obstacles, line, stop sign and AprilTag, whereas real world comprises various other objects/people with different materials and lighting conditions.
- PID controller gains will be different for simulation and real-world deployments since [Gazebo](https://gazebosim.org/home) simulator does not capture "truly" realistic dynamics of the robot and its environment.
- Although uncompressed image transport might be acceptable for simulation, compressed image transport is highly recommended for real-world deployments in order to reduce on-board computational burden and over-the-air image transport latency significantly.
- Initial (first-instance) data availability on subscribed topics is verified (recursion is observed to be a computational burden), however, intermittent data losses and undiscoverable topics pose major issues in wireless ROS-2 framework (specifically due to stringent QoS requirements for sensors and actuators).
- Robot motion constraints (`max_lin_vel = 0.22 m/s` and `max_ang_vel = 2.84 rad/s`) need to be imposed on the controller to ensure bounded control input publication (especially for simulations since [Gazebo simulator](https://gazebosim.org/home) does not impose any such constraints on the robot).

## Troubleshooting Tips:

- In case [Gazebo simulator](https://gazebosim.org/home) crashes, hangs or ceases to launch properly, try killing all instances of `gzserver` and `gzclient` using the following command:
    ```bash
    user@computer:~$ killall gzserver and killall gzclient
    ```
- [Problem of inavailability of data on topics or undiscoverable nodes](https://answers.ros.org/question/372464/ros2-foxy-cant-discover-nodes-from-other-machines) itself (mostly the case with distributed-networked real-world deployment) can be potentially tackled by using a dedicated network isolated of any other open access points in proximity (to avoid [wireless overlap and interference](https://www.makeuseof.com/tag/fix-slow-unstable-wi-fi-connection) from other networks or devices).

## Dependencies:

- [TurtleBot3 Burger Robot Hardware](https://www.robotis.us/turtlebot-3-burger-us/) with [TurtleBot3 SBC Image](https://emanual.robotis.com/docs/en/platform/turtlebot3/sbc_setup/)
- [ROS2 Foxy Fitzroy](https://docs.ros.org/en/foxy/Installation/Alternatives/Ubuntu-Development-Setup.html) on [Ubuntu 20.04 Focal Fossa](https://releases.ubuntu.com/focal/)
- [TurtleBot3 Packages](https://github.com/ROBOTIS-GIT/turtlebot3/tree/foxy-devel) - Included with this repository
- [TurtleBot3 Simulations Packages](https://github.com/ROBOTIS-GIT/turtlebot3_simulations/tree/foxy-devel) - Included with this repository
- [TurtleBot3 Messages Package](https://github.com/ROBOTIS-GIT/turtlebot3_msgs/tree/foxy-devel) - Included with this repository
- [TurtleBot3 Dynamixel SDK Packages](https://github.com/ROBOTIS-GIT/DynamixelSDK/tree/foxy-devel) - Included with this repository

## Build:

1. Make a directory `ROS2_WS` to act as your ROS2 workspace.
    ```bash
    $ mkdir -p ~/ROS2_WS/src/
    ```
2. Clone this repository:
    ```bash
    $ git clone https://github.com/Tinker-Twins/Autonomy-Science-And-Systems.git
    ```
3. Move `capstone_project` directory with required ROS2 packages to the source space (`src`) of your `ROS2_WS`.
    ```bash
    $ mv ~/Autonomy-Science-And-Systems/Capstone\ Project/capstone_project/ ~/ROS2_WS/src/
    ```
4. [Optional] Remove the unnecessary files.
    ```bash
    $ sudo rm -r Autonomy-Science-And-Systems
    ```
5. Build the packages.
    ```bash
    $ cd ~/ROS2_WS
    $ colcon build
    ```
6. Source the `setup.bash` file of your `ROS2_WS`.
    ```bash
    $ echo "source ~/ROS2_WS/install/setup.bash" >> ~/.bashrc
    $ source ~/.bashrc
    ```

## Execute:

### Simulation:
1. Capstone Project:
    ```bash
    user@computer:~$ ros2 launch capstone_project capstone_project_sim.launch.py
    ```
2. Wall Following:
    ```bash
    user@computer:~$ ros2 launch capstone_project wall_following_sim.launch.py
    ```
3. Obstacle Avoidance:
    ```bash
    user@computer:~$ ros2 launch capstone_project obstacle_avoidance_sim.launch.py
    ```
4. Line Following:
    ```bash
    user@computer:~$ ros2 launch capstone_project line_following_sim.launch.py
    ```
5. Stop Sign Detection:
    ```bash
    user@computer:~$ ros2 launch capstone_project stop_sign_detection_sim.launch.py
    ```
6. AprilTag Tracking:
    ```bash
    user@computer:~$ ros2 launch capstone_project apriltag_tracking_sim.launch.py
    user@computer:~$ ros2 run capstone_project apriltag_teleop
    ```

### Real World:
1. Connect to the TurtleBot3 SBC via Secure Shell Protocol (SSH):
    ```bash
    user@computer:~$ sudo ssh <username>@<ip.address.of.turtlebot3>
    user@computer:~$ sudo ssh ubuntu@192.168.43.48
    ```
2. Bringup TurtleBot3:
    ```bash
    ubuntu@ubuntu:~$ ros2 launch turtlebot3_bringup robot.launch.py
    ubuntu@ubuntu:~$ ros2 launch v4l2_camera camera.launch.py
    ```
3. Capstone Project:
    ```bash
    user@computer:~$ ros2 launch capstone_project capstone_project_real.launch.py
    ```
4. Wall Following:
    ```bash
    user@computer:~$ ros2 launch capstone_project wall_following_real.launch.py
    ```
5. Obstacle Avoidance:
    ```bash
    user@computer:~$ ros2 launch capstone_project obstacle_avoidance_real.launch.py
    ```
6. Line Following:
    ```bash
    user@computer:~$ ros2 launch capstone_project line_following_real.launch.py
    ```
7. Stop Sign Detection:
    ```bash
    user@computer:~$ ros2 launch capstone_project stop_sign_detection_real.launch.py
    ```
8. AprilTag Tracking:
    ```bash
    user@computer:~$ ros2 launch capstone_project apriltag_tracking_real.launch.py
    ```

## Results:

The [`media`](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/tree/main/Capstone%20Project/media) directory hosts pictures and videos of the implementations.

1. Simulation:

![Capstone Project Sim](media/capstone_project_sim_1.gif)

2. Real World:

![Capstone Project Real](media/capstone_project_real_1.gif)

3. Robustness Testing:

| ![Wall Following Simulation](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_sim_1.gif) | ![Wall Following Robot](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_sim_2.gif) | ![Lane Following RViz](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_sim_3.gif) | ![Lane Following RViz](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_sim_4.gif) |
|:---------------------------------:|:------------------------------------:|:------------------------------------:|:-----------------------------------:|
| Simulation: Forward Direction; Start at Wall Following | Simulation: Forward Direction; Start at Line Following | Simulation: Forward Direction; Start at AprilTag Tracking | Simulation: Inverse Direction; Start at Line Following |
| ![Wall Following Simulation](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_real_1.gif) | ![Wall Following Robot](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_real_2.gif) | ![Lane Following RViz](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_real_3.gif) | ![Lane Following RViz](https://github.com/Tinker-Twins/Autonomy-Science-And-Systems/blob/main/Capstone%20Project/media/capstone_project_real_4.gif) |
| Real-World: Forward Direction; Day; Start at Wall Following | Real-World: Forward Direction; Night; Start at Line Following | Real-World: Forward Direction; Night; Start at AprilTag Tracking | Real-World: Inverse Direction; Night; Start at Line Following |

4. Individual Tasks:

| ![Wall Following Simulation](media/wall_following_sim.gif) | ![Wall Following Robot](media/wall_following_real_robot.gif) | ![Lane Following RViz](media/wall_following_real_rviz.gif) |
|:---------------------------------:|:------------------------------------:|:------------------------------------:|
| Wall Following - Simulation | Wall Following - TurtleBot3 | Wall Following - Remote PC |
| ![Obstacle Avoidance Simulation](media/obstacle_avoidance_sim.gif) | ![Obstacle Avoidance Robot](media/obstacle_avoidance_real_robot.gif) | ![Obstacle Avoidance RViz](media/obstacle_avoidance_real_rviz.gif) |
| Obstacle Avoidance - Simulation | Obstacle Avoidance - TurtleBot3 | Obstacle Avoidance - Remote PC |
| ![Line Following Simulation](media/line_following_sim.gif) | ![Line Following Robot](media/line_following_real_robot.gif) | ![Lane Following RViz](media/line_following_real_rviz.gif) |
| Line Following - Simulation | Line Following - TurtleBot3 | Line Following - Remote PC |
| ![Stop Sign Detection Simulation](media/stop_sign_detection_sim.gif) | ![Stop Sign Detection Robot](media/stop_sign_detection_real_robot.gif) | ![Stop Sign Detection RViz](media/stop_sign_detection_real_rviz.gif) |
| Stop Sign Detection - Simulation | Stop Sign Detection - TurtleBot3 | Stop Sign Detection - Remote PC |
| ![AprilTag Tracking Simulation](media/apriltag_tracking_sim.gif) | ![AprilTag Tracking Robot](media/apriltag_tracking_real_robot.gif) | ![AprilTag Tracking RViz](media/apriltag_tracking_real_rviz.gif) |
| AprilTag Tracking - Simulation | AprilTag Tracking - TurtleBot3 | AprilTag Tracking - Remote PC |
